{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)\n",
    "\n",
    "1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J(theta)是模型的cost functiony是实际数据y_hat是你的模型推算出来的数据theta是模型的参量\n",
    "最原始的regression:J(theta) ＝ sum((y - y_hat)^2)\n",
    "加上L1(也就是LASSO):J_lasso(theta) = sum((y - y_hat)^2) + sum(abs(theta))\n",
    "加上L2(也就是Ridge):J_ridge(theta) = sum((y - y_hat)^2) + sum(theta^2)\n",
    "\n",
    "L1和L2什么时候用呢？\n",
    "如果你有很多features，不知道哪个最重要，那么你可以用L1，因为L1会更鼓励theta为0。这样你可以直接用非0的theta，那么模型的复杂程度会降低很多\n",
    "\n",
    "![](https://pic4.zhimg.com/c822c410dd5132724d7c10d5381cfd7c_r.jpg)\n",
    "那个长得像年轮蛋糕的圈圈就是J(theta)，左边的圆圈是L2，右边的方块是L1。也就是说你在做最优化的时候，你在原有的空间里叠加了一个圆圈／方块，这样就会有一种“力”把你的theta向0拉，这也是为什么叫LASSO的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression应用在反应变量和观测量成线性关系加一个高斯误差的模型上。怎么估计参数？最基础常见的是least square，其结果的好处在于无偏。但是当观测量X接近singular的时候，least square估计量的方差大。ridge是在最优化的函数上做l2的regularization，而lasso是l1的。ridge是修改优化函数后的升级版regression。而lasso最大的好处在于，curve常和数轴上的顶点相交，自动完成了feature selection，不用再看显著性的脸色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于ridge， 损失函数为：sum (y - bx)^2 + gamma b^2,根据KKT条件，b需要满足一阶导数为0，即：sum 2 (y - bx) x = 2 gamma b,其中，等式左边为b对loss的边际效用， 右边是b关于惩罚项的边际效用。在ridge回归中，我们要求，如果参数b越大，它对loss的边际贡献应该更大（2 gamma b）。同样，如果参数b很小，我们只要求它对loss有较小的边际贡献。 因此，通常在ridge中，b不会被优化为0，因为对于较小的b，我们只期望它有较小的边际贡献。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/weixin_43374551/article/details/83688913?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着模型复杂度的提升，在训练集上的效果就越好，即模型的偏差就越小；但是同时模型的方差就越大。对于岭回归的λ\\lambdaλ而言，随着λ\\lambdaλ的增大，∣XTX+λI∣|X^TX+\\lambda I|∣X T X+λI∣就越大，(XTX+λI)−1(X^TX+\\lambda I)^{-1}(X T X+λI)−1 就越小，模型的方差就越小；而λ\\lambdaλ越大使得β\\betaβ的估计值更加偏离真实值，模型的偏差就越大。所以岭回归的关键是找到一个合理的λ\\lambdaλ值来平衡模型的方差和偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
